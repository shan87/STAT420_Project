---
title: "STAT 420: Data Analysis Project - Predict the price of your dream home" 
author: "Anupama Agrahari, Dhanendra Singh, Naveen Kumar Palani, Shanthakumar Subramanian"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---




## Team

We are team of four. Please find below our names and Net ID in alphabetical order.

```{r echo = FALSE, result = 'asis'}
members = matrix(nrow = 4, ncol = 3)
colnames(members) = c("FirstName", "LastName", "NetID")
rownames(members) = c("1", "2", "3","4")

members[1,1] = "Anupama"
members[1,2] = "Agrahari"
members[1,3] = "anupama3"
members[2,1] = "Dhanendra"
members[2,2] = "Singh"
members[2,3] = "disingh2"
members[3,1] = "Naveen Kumar"
members[3,2] = "Palani"
members[3,3] = "npalani3"
members[4,1] = "Shanthakumar"
members[4,2] = "Subramanian"
members[4,3] = "SS81"
knitr::kable(members)
```

## Introduction

### About the Project
This data analysis project is inspired from the House Prices: Advanced Regression Techniques competition held in Kaggle.

In this project we will build a model to predict the SalePrice of each home, based on the 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.

###Data Set

#### Source
Below are the details about our dataset. We are using house prie data available at  https://www.kaggle.com/c/house-prices-advanced-regression-techniques . We will use regression techniques to predict the SalePrice. 

#### Background

The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. 

#### File descriptions

List of files used in the project

- train.csv - the training set
- test.csv - the test set
- data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here

#### Data fields

Here's a brief description of different fields of the house data set.

- SalePrice - The property's sale price in dollars. This is the target variable that we are trying to predict.
- MSSubClass: The building class
- MSZoning: The general zoning classification
- LotFrontage: Linear feet of street connected to property
- LotArea: Lot size in square feet
- Street: Type of road access
- Alley: Type of alley access
- LotShape: General shape of property
- LandContour: Flatness of the property
- Utilities: Type of utilities available
- LotConfig: Lot configuration
- LandSlope: Slope of property
- Neighborhood: Physical locations within Ames city limits
- Condition1: Proximity to main road or railroad
- Condition2: Proximity to main road or railroad (if a second is present)
- BldgType: Type of dwelling
- HouseStyle: Style of dwelling
- OverallQual: Overall material and finish quality
- OverallCond: Overall condition rating
- YearBuilt: Original construction date
- YearRemodAdd: Remodel date
- RoofStyle: Type of roof
- RoofMatl: Roof material
- Exterior1st: Exterior covering on house
- Exterior2nd: Exterior covering on house (if more than one material)
- MasVnrType: Masonry veneer type
- MasVnrArea: Masonry veneer area in square feet
- ExterQual: Exterior material quality
- ExterCond: Present condition of the material on the exterior
- Foundation: Type of foundation
- BsmtQual: Height of the basement
- BsmtCond: General condition of the basement
- BsmtExposure: Walkout or garden level basement walls
- BsmtFinType1: Quality of basement finished area
- BsmtFinSF1: Type 1 finished square feet
- BsmtFinType2: Quality of second finished area (if present)
- BsmtFinSF2: Type 2 finished square feet
- BsmtUnfSF: Unfinished square feet of basement area
- TotalBsmtSF: Total square feet of basement area
- Heating: Type of heating
- HeatingQC: Heating quality and condition
- CentralAir: Central air conditioning
- Electrical: Electrical system
- X1stFlrSF: First Floor square feet
- X2ndFlrSF: Second floor square feet
- LowQualFinSF: Low quality finished square feet (all floors)
- GrLivArea: Above grade (ground) living area square feet
- BsmtFullBath: Basement full bathrooms
- BsmtHalfBath: Basement half bathrooms
- FullBath: Full bathrooms above grade
- HalfBath: Half baths above grade
- Bedroom: Number of bedrooms above basement level
- Kitchen: Number of kitchens
- KitchenQual: Kitchen quality
- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
- Functional: Home functionality rating
- Fireplaces: Number of fireplaces
- FireplaceQu: Fireplace quality
- GarageType: Garage location
- GarageYrBlt: Year garage was built
- GarageFinish: Interior finish of the garage
- GarageCars: Size of garage in car capacity
- GarageArea: Size of garage in square feet
- GarageQual: Garage quality
- GarageCond: Garage condition
- PavedDrive: Paved driveway
- WoodDeckSF: Wood deck area in square feet
- OpenPorchSF: Open porch area in square feet
- EnclosedPorch: Enclosed porch area in square feet
- 3SsnPorch: Three season porch area in square feet
- ScreenPorch: Screen porch area in square feet
- PoolArea: Pool area in square feet
- PoolQC: Pool quality
- Fence: Fence quality
- MiscFeature: Miscellaneous feature not covered in other categories
- MiscVal: $Value of miscellaneous feature
- MoSold: Month Sold
- YrSold: Year Sold
- SaleType: Type of sale
- SaleCondition: Condition of sale

## Data Preparation

### Data Import

```{r,message=FALSE,error=FALSE, warning=FALSE}

#Libraries used

library(lmtest)
library(broom)
library(formattable)
library(faraway)
library(ggplot2)
library(leaps)
library(caret)

```

**Importing test and train data from the files**

```{r}

#Import train and test dataset from the source files.

train = read.csv("train.csv", stringsAsFactors = FALSE) 
test = read.csv("test.csv", stringsAsFactors = FALSE)

```

**Checking the number of columns and observation in test and train data**

```{r}

dim(train)
dim(test)

```

From the dimension of test and train data, we could see that the test data has one less column than the train, as the **Sale Price** column would not be availble in test data and that's what we will be predicting as part of this project.

###Data Cleansing

```{r}

#Displaying the structure of train dataset

str(train)

```

From the above,`str` display of data, we could see the below issues:

-  The data is not completely clean, certain variables contain `NA` values which could cause problems when we do any mathematical operation and while fitting the model. 
- The levels of some of the factor variables are not the same across the training set and test set, which might also cause some problems while fitting the model.
- Some of the categoricals variables are listed as numeric, this might have some impact on fitting the model.

We will do the following to rectify the above issues:

- Remove NA and update the values with "None" for character variables.
- Impute/replce NA with either `0`, `Mean`, `Medain` or `Mode` for numeric variables appropriatley.


We should make sure that, the train and test sets have the same factor levels by loading each data set again without converting strings to factors, combining them into one large data set, converting strings to factors for the combined data set and then separating them. 


We also can change the data types of the below features from numeric to string.

`MSSubClass`, `OverallCond`, `OverallQual`, `GarageCars`, `YrSold`, `MoSold`

The values for each feature above represent different categories and not different amounts of something. This is easiest to see in the case of MSSubClass, where the numbers encode different categories of houses, such as 2-Story 1946 and Newer (60) and 2-Story 1945 and Older (70).  It is harder to discern in a feature like GarageCars, where each value seems to count something (cars) but in actuality represents the garage capacity, and therefore represents a category.

####Step 1

Removing the target variable SalePrice, which is not found in test data set.

```{r}

## Removing the target variable saleprice

SalePrice = train$SalePrice 
train$SalePrice = NULL

```

####Step 2

Combining test and training data set and changing numeric to factor variable.

```{r}
# Combine data sets

house_data = rbind(train,test)

##Change Numeric to factor

house_data$MSSubClass = as.character(house_data$MSSubClass)
house_data$OverallCond = as.character(house_data$OverallCond)
house_data$OverallQual = as.character(house_data$OverallQual)
house_data$GarageCars = as.character(house_data$GarageCars)
house_data$YrSold = as.character(house_data$YrSold)
house_data$MoSold = as.character(house_data$MoSold)

```

####Step 3

We will now convert `character` columns to factor and will replace NA values with "None".

```{r}

## Replacing the NA in character variable with None.

for (col in colnames(house_data)){
  if (typeof(house_data[,col]) == "character"){
    new_col = house_data[,col]
    new_col[is.na(new_col)] = "None"
    house_data[col] = as.factor(new_col)
  }
}
```

Now the `factor variable` should be common across test and train dataset.

####Step 4

Lets try to find the missing values in `integer variable` and fix it.

```{r}

## Checking for missing data values in all integer variables

Missing_indices = sapply(house_data,function(x) sum(is.na(x)))
Missing_Summary = data.frame(index = names(house_data),Missing_Values=Missing_indices)
Missing_Summary[Missing_Summary$Missing_Values > 0,]
```

Above are the list of variable having NA values, in the below section, we will fix those values with relavant values

- Imputed missing values of `MasVnrArea` with its `mean`.
- Imputed missing values of `LotFrontage` with its `median`
- Imputed the missing values of below with `0`.
  
  - `GarageYrBlt`, `BsmtFullBath`, `BsmtHalfBath`, `BsmtUnfSF`, `BsmtFinSF1`, `BsmtFinSF2`, `GarageArea`, `GarageCars`, `TotalBsmtSF`
      

```{r}

## Impute Mean
house_data$MasVnrArea[which(is.na(house_data$MasVnrArea))] = mean(house_data$MasVnrArea,na.rm=T)

## Impute Median

house_data$LotFrontage[which(is.na(house_data$LotFrontage))] = median(house_data$LotFrontage,na.rm = T)

## Impute 0

house_data$GarageYrBlt[which(is.na(house_data$GarageYrBlt))] = 0 

house_data$BsmtFullBath[which(is.na(house_data$BsmtFullBath ))] = 0

house_data$BsmtHalfBath[which(is.na(house_data$BsmtHalfBath ))] = 0

house_data$BsmtUnfSF[which(is.na(house_data$BsmtUnfSF))] = 0

house_data$BsmtFinSF1[which(is.na(house_data$BsmtFinSF1))] = 0

house_data$BsmtFinSF2[which(is.na(house_data$BsmtFinSF2))] = 0

house_data$GarageArea[which(is.na(house_data$GarageArea))] = 0

house_data$GarageCars[which(is.na(house_data$GarageCars))] = 0

house_data$TotalBsmtSF[which(is.na(house_data$TotalBsmtSF))] = 0

```

####Step 5

We will split the train and test data now as the data issues are fixed.

```{r}
# Separate out our train and test sets

train = house_data[1:nrow(train),]
train$SalePrice = SalePrice  
test = house_data[(nrow(train)+1):nrow(house_data),]
```

Let's now have a look at the train data set

```{r}

#Displaying the train dataset structure after data cleansing.

str(train)

```

From the above `str` of train dataset, we could see that the `NA` in the dataset is completely removed now.

###Train and Test data Split

We can split the data from the train dataset in 80 - 20 ratio, we can use this 20% data of train dataset to test our fitted model.

```{r}

#Splitting the train dataset into train and test

set.seed(1)
house_trn_idx = sample(1:nrow(train), 1168)
house_trn = train[house_trn_idx,]
house_tst = train[-house_trn_idx,]

```

## Exploratory Data Analysis

### Correlation Matrix

Lets plot the correlation matrix to see which variables have multicollinearity.

```{r fig.height=8, fig.width=13, warning=FALSE}

# Correlation Plot

house_train_num<-house_trn[, sapply(house_trn, is.numeric)]
Correlation<-cor(na.omit(house_train_num))
library(corrplot)
corrplot(Correlation, method = "square")

```

From the above plot, we could see that some varaiables have high correlation between each other, below are the few examples:

- TotalBsmtSF and X1stFlrSF
- BsmtFinSF1 and BsmtFullBath
- X2ndFlrSF and GrLivArea

### Correlation with SalePrice

We will explore the Variable which has correlation of more than `0.5` with SalePrice.

```{r}

#variables having Correlation > 0.5 with SalePrice  

for (col in colnames(house_trn)){
   if(is.numeric(house_trn[,col])){
       if( abs(cor(house_trn[,col],house_trn$SalePrice)) > 0.5){
           print(col)
           print( cor(house_trn[,col],house_trn$SalePrice))
         }
   }
}

```


We can use the variable listed above to fit our model as it has high corelation with sales price.


###Distribtion of SalePrice

```{r fig.width=10}

#Distribution of SalePrice

par(mfrow = c(1,2))

hist(house_trn$SalePrice,
    xlab   = "Sale Price without log(SalePrice)",
    main   = "Histogram of sales price",
    border = "blue",
    breaks = 20)

#Distribution of log(SalePrice)

hist(log(house_trn$SalePrice),
    xlab   = "Sale Price with log(SalePrice)",
    main   = "Histogram of sales price",
    border = "blue",
    breaks = 20)

#Q-Q Plot for SalePrice

qqnorm(house_trn$SalePrice, main = "Normal Q-Q Plot, without log(SalePrice)", col = "darkgrey")
qqline(house_trn$SalePrice, col = "blue", lwd = 2)

#Q-Q Plot for log(SalePrice)

qqnorm(log(house_trn$SalePrice), main = "Normal Q-Q Plot, with log(SalePrice) ", col = "darkgrey")
qqline(log(house_trn$SalePrice), col = "blue", lwd = 2)

```

From the above plot, we could see that the SalePrice data is `right-skewed` so we can apply log transformation for SalePrice.

After applying `log transformation`, we could see that the SalePrice is `normally distributed`.

### Distribution of numeric varibles

In this section, we will explore the distribution of numeric variables which have high correlation with SalePrice.

```{r fig.height=3, fig.width=10}

#Histogram for checking the distribution of numeric variables

var_sel = c("YearBuilt", "YearRemodAdd","TotalBsmtSF", "X1stFlrSF","GrLivArea", "FullBath", "TotRmsAbvGrd", "GarageArea")

par(mfrow = c(1,3))
for(i in 1:length(var_sel))
hist(house_trn[,var_sel[i]],
    xlab   = paste(toString(var_sel[i])),
    main   = paste("Histogram of ",toString(var_sel[i])),
    border = "blue",
    breaks = 20)

```

From the above plots, we can infer the following
- TotalBsmtSF is `right-skewed`
- GrLivArea is `right-skewed`
- X1stFlrSF is `right-skewed`
- GarageArea is `right-skewed`
- YearBuilt is `left-skewed`

When we use the these variables which has right-skewedness, we can apply log transformation for these variables.

### SalePrice Vs Factor Variables

In this section, we can explore the effect of each factor variable on SalePrice.

```{r fig.height=5, fig.width=15}

#Box plot for factor variables against SalePrice 

char_var = as.array(names(house_trn[, sapply(house_trn, class) == 'factor']))

par(mfrow = c(1,2))
for(i in 1:length(char_var))
boxplot(house_trn$SalePrice ~ house_trn[,char_var[i]], data = mpg,
     xlab   = paste(toString(char_var[i])),
     ylab   = "Sales price",
     main   = paste("saleprice vs", toString(char_var[i])),
     pch    = 20,
     cex    = 2,
     las=1,
     outlier.size=10,
     col    = "blue",
     border = "Black")

```

From the above plots we could see that the below variable have significance on SalePrice.

- MSSubClass
- MSZoning
- Neighborhood
- OverallQual
- KitchenQual


## Model Building

Based on the above data exploration, we will start building the model with the variables which has high significane on Saleprice.

We will also put a log transformation for the SalePrice as it has right skewedness, which we infered from the histogram plot in the data exploration section.

### Additive Model

```{r}

add_mod = lm(log(SalePrice) ~  (MSSubClass +GrLivArea + TotalBsmtSF + OverallQual+ YearBuilt+ FullBath + YearRemodAdd + Neighborhood +TotRmsAbvGrd+ExterQual), data = house_trn)

```

**R-Squared**

```{r}

(r2_add=summary(add_mod)$r.squared)

```

**Q-Q and Fitted vs Residual Plot**

```{r fig.height=6, fig.width=13}

par(mfrow = c(1, 2))
plot(fitted(add_mod), resid(add_mod), col = "grey", pch = 20,ylim = c(-1, 1),
    xlab = "Fitted", ylab = "Residuals", main = "Data from Additive Model-1")
abline(h = 0, col = "dodgerblue", lwd = 2)

qqnorm(resid(add_mod), main = "Normal Q-Q Plot", col = "darkgrey",ylim = c(-1, 1))
qqline(resid(add_mod), col = "dodgerblue", lwd = 2)

```

**BP and Shapiro Test **

```{r}

bptest(add_mod)
shapiro.test(resid(add_mod))

```

**Test and Train RMSE**

```{r,warning=FALSE}

 (RMSE_Train_add = sqrt(mean(resid(add_mod) ^ 2)))
 
 (RMSE_test_add = sqrt(mean((log(house_tst$SalePrice) - predict(add_mod,house_tst)) ^ 2)))

```

**Discussion on test results for Additive model**

The above additive model have the following test results

- $R^2$ value of `r round(summary(add_mod)$r.squared,4)`, ie `r percent(summary(add_mod)$r.squared)` of the variance in SalePrice is explained by linear relationship with variables in the `additive model`.

- The model is `violating the constant variance and normality assumptions` which we could see from the QQ plot and fitted vs residual plot.The p values of `BP test` and `Shapiro-wilk test` also confirms the same.

- Train RMSE of `r round((RMSE_Train_add = sqrt(mean(resid(add_mod) ^ 2))),4)`

- Test RMSE of `r round((RMSE_test_add = sqrt(mean((log(house_tst$SalePrice) - predict(add_mod,house_tst)) ^ 2))),4)`

This model is violating the model assumptions and have $R^2$ value of `r round(summary(add_mod)$r.squared,4)`, we will do further analysis to find a model which doesn't violate the model assumptions and have better $R^2$.


### Additive Model 2

In the above additive model, few variables like MSSubclass which doesn't have significant p-value, so we will remove those non-significant variables and fit another additive model.

```{r}

add_mod2 = lm(log(SalePrice) ~  (GrLivArea + TotalBsmtSF + OverallQual+ YearBuilt+ YearRemodAdd + Neighborhood +TotRmsAbvGrd+ExterQual), data = house_trn)

```

**R-Squared**

```{r}

(r2_add2=summary(add_mod2)$r.squared)

```

**Q-Q and Fitted vs Residual Plot**

```{r fig.height=6, fig.width=13}

par(mfrow = c(1, 2))
plot(fitted(add_mod2), resid(add_mod2), col = "grey", pch = 20,ylim = c(-1, 1),
    xlab = "Fitted", ylab = "Residuals", main = "Data from Additive Model-2")
abline(h = 0, col = "dodgerblue", lwd = 2)

qqnorm(resid(add_mod2), main = "Normal Q-Q Plot", col = "darkgrey",ylim = c(-1, 1))
qqline(resid(add_mod2), col = "dodgerblue", lwd = 2)

```

**BP and Shapiro Test **

```{r}

bptest(add_mod2)
shapiro.test(resid(add_mod2))

```

**Test and Train RMSE**

```{r,warning=FALSE}

 (RMSE_Train_add2 = sqrt(mean(resid(add_mod2) ^ 2)))
 
 (RMSE_test_add2 = sqrt(mean((log(house_tst$SalePrice) - predict(add_mod2,house_tst)) ^ 2)))

```

**Discussion on test results for Additive model-2**

The above additive model have the following test results

- $R^2$ value of `r round(summary(add_mod2)$r.squared,4)`, ie `r percent(summary(add_mod2)$r.squared)` of the variance in SalePrice is explained by linear relationship with variables in the `additive model`.

- The model is `violating the constant variance and normality assumptions` which we could see from the QQ plot and fitted vs residual plot.The p values of `BP test` and `Shapiro-wilk test` also confirms the same.

- Train RMSE of `r round((RMSE_Train_add = sqrt(mean(resid(add_mod2) ^ 2))),4)`

- Test RMSE of `r round((RMSE_test_add = sqrt(mean((log(house_tst$SalePrice) - predict(add_mod2,house_tst)) ^ 2))),4)`

This model is also violating the model assumptions and have $R^2$ value of `r round(summary(add_mod2)$r.squared,4)`, which is lower than the previous model.We will do further analysis to find a model which doesn't violate the model assumptions and have better $R^2$.


#### Anova of Additive Model 1 and Model 2

```{r}

anova(add_mod2,add_mod)

```

The annova test has low p-vlaue of `r tidy(anova(add_mod2,add_mod))[2, "p.value"]`, which is confirming that the variable which we removed in additive model 2 is not significant so we can continue to find a better model with the variables in additive model-2.

### Log Interaction Model

The constant variance and normality assumptions are violated in the above 2 additive models. The reason for this could be that the variables have righ-skewedness, as we commonly say in statistics that the **garbage in, garbage out**.

So we will try to fix the right-skewedness by applying log transformation for the predictor variables.

We will also try some interaction and see, if the $R^2$ is improving.


```{r,warning=FALSE}

log_int_mod = lm(log(SalePrice) ~  (log(GrLivArea)+OverallQual+ YearBuilt+ YearRemodAdd + Neighborhood +log(TotRmsAbvGrd)+ExterQual)^2, data = house_trn)

```


**R-Squared**

```{r}

(r2_int=summary(log_int_mod)$r.squared)

```



**Q-Q and Fitted vs Residual Plot**

```{r fig.height=6, fig.width=13}

par(mfrow = c(1, 2))
plot(fitted(log_int_mod), resid(log_int_mod), col = "grey", pch = 20,ylim = c(-1, 1),
    xlab = "Fitted", ylab = "Residuals", main = "Data from Log interaction model")
abline(h = 0, col = "dodgerblue", lwd = 2)

qqnorm(resid(log_int_mod), main = "Normal Q-Q Plot", col = "darkgrey",ylim = c(-1, 1))
qqline(resid(log_int_mod), col = "dodgerblue", lwd = 2)

```

**BP and Shapiro Test **

```{r}

bptest(log_int_mod)
shapiro.test(resid(log_int_mod))

```

**Test and Train RMSE**

```{r,warning=FALSE}

 (RMSE_Train_log_int = sqrt(mean(resid(log_int_mod) ^ 2)))
 
 (RMSE_test_log_int = sqrt(mean((log(house_tst$SalePrice) - predict(log_int_mod,house_tst)) ^ 2)))

```

**Discussion on test results for Log interaction Model**

The above log interaction model have the following test results

- $R^2$ value of `r round(summary(log_int_mod)$r.squared,4)`, ie `r percent(summary(log_int_mod)$r.squared)` of the variance in SalePrice is explained by linear relationship with variables in the `additive model`.

- The model is `violating the normality assumptions` but the `constant variance assumption is valid` which we could see from the QQ plot and fitted vs residual plot.The high p-value of `BP test` confirms the constant variance and low p-value of `Shapiro-wilk test` shows that the normality assumption is invalid.

- Train RMSE of `r round((RMSE_Train_add = sqrt(mean(resid(log_int_mod) ^ 2))),4)`

- Test RMSE of `r round((RMSE_test_add = sqrt(mean((log(house_tst$SalePrice) - predict(log_int_mod,house_tst)) ^ 2))),4)`

Even though the constant variance is valid for the log interaction model, its still violating the normality assumption and we see better $R^2$ value of `r round(summary(log_int_mod)$r.squared,4)` compared to the other 2 additive models.

We will do further analysis and try to fit a model which doesn't violate the model assumptions and still have better $R^2$ as this model.


### Log Interation Model without Influential data

To fix the normality assumption violation, we wil remove the influential observations and fit the above model.

```{r,warning=FALSE}

#Log Intearction Model

log_int_mod = lm(log(SalePrice) ~  (log(GrLivArea)+OverallQual+ YearBuilt+ YearRemodAdd + Neighborhood +log(TotRmsAbvGrd)+ExterQual)^2, data = house_trn)

#Cooks distance for log interaction model

log_int_mod_cd = cooks.distance(log_int_mod)

#Log interaction model after removing influential points
   
 int_add_cd_mod = lm(log(SalePrice) ~  (log(GrLivArea)+OverallQual+YearBuilt + YearRemodAdd + Neighborhood +log(TotRmsAbvGrd)+ExterQual)^2, data = house_trn  , subset = log_int_mod_cd < 4 / length(log_int_mod_cd))
 
```

**R-Squared**

```{r}

(r2_cd=summary(int_add_cd_mod)$r.squared)

```


**Q-Q and Fitted vs Residual Plot** 
 
 
```{r fig.height=6, fig.width=13}
 
 par(mfrow = c(1, 2))
plot(fitted(int_add_cd_mod), resid(int_add_cd_mod), col = "grey", pch = 20,ylim = c(-1, 1),
    xlab = "Fitted", ylab = "Residuals", main = "Log interaction model - influential removed")
abline(h = 0, col = "dodgerblue", lwd = 2)

qqnorm(resid(int_add_cd_mod), main = "Normal Q-Q Plot", col = "darkgrey",ylim = c(-1, 1))
qqline(resid(int_add_cd_mod), col = "dodgerblue", lwd = 2)

```


**BP and Shapiro Test **

```{r}

bptest(int_add_cd_mod)
shapiro.test(resid(int_add_cd_mod))

```

**Test and Train RMSE**

```{r,warning=FALSE}
   
(RMSE_Train_log_cd = sqrt(mean(resid(int_add_cd_mod) ^ 2)))
 
 house_mod = subset(house_tst, house_tst$OverallQual != "2" & house_tst$Neighborhood !="Blueste" & house_tst$Neighborhood !="Veenker" & house_tst$ExterQual !="Fa" )
 
(RMSE_test_log_cd = sqrt(mean((log(house_tst$SalePrice) - predict(int_add_cd_mod,house_mod)) ^ 2)))
 
 
```

**Discussion on test results - Log interaction model with influential points removed**

The above log interaction model without influential points have the following test results

- $R^2$ value of `r round(summary(int_add_cd_mod)$r.squared,4)`, ie `r percent(summary(int_add_cd_mod)$r.squared)` of the variance in SalePrice is explained by linear relationship with variables in the `additive model`.

- Normality assumptions and the constant variance assumptions is **valid** for this model which we could see from the QQ plot and fitted vs residual plot.The high p-value of `BP test` and `Shapiro-wilk test` confirms the same.

- Train RMSE of `r round((RMSE_Train_add = sqrt(mean(resid(int_add_cd_mod) ^ 2))),4)`

- Test RMSE of `r round((RMSE_test_add = sqrt(mean((log(house_tst$SalePrice) - predict(int_add_cd_mod,house_mod)) ^ 2))),4)`

This model has improved $R^2$ value and the model assumptions are also valid but there `r length(coef(int_add_cd_mod))` parameters, which will make the model interpretation difficult.

Now, we will focus on **reducing the model parameters** by using the AIC/BIC search methods.


### BIC of Log Interation Model

In this section, we will do a BIC step search to reduce the number of parameters.

```{r,warning=FALSE}

n = length(resid(int_add_cd_mod))

BIC_Log_cd = step(int_add_cd_mod,k=log(n),trace=0)
 
```

**R-Squared**

```{r}

(r2_bic=summary(BIC_Log_cd)$r.squared)

```


**Q-Q and Fitted vs Residual Plot** 
 
 
```{r fig.height=6, fig.width=13}
 
 par(mfrow = c(1, 2))
plot(fitted(BIC_Log_cd), resid(BIC_Log_cd), col = "grey", pch = 20,ylim = c(-1, 1),
    xlab = "Fitted", ylab = "Residuals", main = "Data from Model 2")
abline(h = 0, col = "dodgerblue", lwd = 2)

qqnorm(resid(BIC_Log_cd), main = "Normal Q-Q Plot", col = "darkgrey",ylim = c(-1, 1))
qqline(resid(BIC_Log_cd), col = "dodgerblue", lwd = 2)

```


**BP and Shapiro Test **

```{r}

bptest(BIC_Log_cd)
shapiro.test(resid(BIC_Log_cd))

```

**Test and Train RMSE**

```{r,warning=FALSE}
   
(RMSE_Train_BIC_log_cd = sqrt(mean(resid(BIC_Log_cd) ^ 2)))
 
 house_mod = subset(house_tst, house_tst$OverallQual != "2" & house_tst$Neighborhood !="Blueste" & house_tst$Neighborhood !="Veenker" & house_tst$ExterQual !="Fa" )
 
(RMSE_test_BIC_log_cd = sqrt(mean((log(house_tst$SalePrice) - predict(BIC_Log_cd,house_mod)) ^ 2)))
 
 
```


**Discussion on test results for BIC search Model**

The above model built using BIC have the following test results

- $R^2$ value of `r round(summary(BIC_Log_cd)$r.squared,4)`, ie `r percent(summary(BIC_Log_cd)$r.squared)` of the variance in SalePrice is explained by linear relationship with variables in the `BIC model`.

- Normality assumptions and the constant variance assumptions are **not valid** for this model which we could see from the QQ plot and fitted vs residual plot.The low p-value of `BP test` and `Shapiro-wilk test` confirms the same.

- Train RMSE of `r round((RMSE_Train_add = sqrt(mean(resid(BIC_Log_cd) ^ 2))),4)`

- Test RMSE of `r round((RMSE_test_add = sqrt(mean((log(house_tst$SalePrice) - predict(BIC_Log_cd,house_mod)) ^ 2))),4)`

This model has reduced $R^2$ value compared to the log interaction model and log interaction without influential points.But this model has `r length(coef(BIC_Log_cd))`parameters which is very less parameters compared to the log interaction model, which make this model interpretable.

Now, we will see if we can use AIC search method to acheive below:

- Less parameters similar to BIC model 
- Better $R^2$ 
- Valid model assumptions and 
- Low train and test RMSE


### AIC of Log Interation Model

In this section, we will do a AIC step search to reduce the number of parameters.

```{r,warning=FALSE}

n = length(resid(int_add_cd_mod))

AIC_Log_cd = step(int_add_cd_mod,trace=0)

```

**R-Squared**

```{r}

(r2_aic=summary(AIC_Log_cd)$r.squared)

```


**Q-Q and Fitted vs Residual Plot** 
 
 
```{r fig.height=6, fig.width=13}
 
 par(mfrow = c(1, 2))
plot(fitted(AIC_Log_cd), resid(AIC_Log_cd), col = "grey", pch = 20,ylim = c(-1, 1),
    xlab = "Fitted", ylab = "Residuals", main = "Data from Model 2")
abline(h = 0, col = "dodgerblue", lwd = 2)

qqnorm(resid(AIC_Log_cd), main = "Normal Q-Q Plot", col = "darkgrey",ylim = c(-1, 1))
qqline(resid(AIC_Log_cd), col = "dodgerblue", lwd = 2)

```


**BP and Shapiro Test **

```{r}

bptest(AIC_Log_cd)
shapiro.test(resid(AIC_Log_cd))

```

**Test and Train RMSE**

```{r,warning=FALSE}
   
(RMSE_Train_AIC_log_cd = sqrt(mean(resid(AIC_Log_cd) ^ 2)))
 
 house_mod = subset(house_tst, house_tst$OverallQual != "2" & house_tst$Neighborhood !="Blueste" & house_tst$Neighborhood !="Veenker" & house_tst$ExterQual !="Fa" )
 
(RMSE_test_AIC_log_cd = sqrt(mean((log(house_tst$SalePrice) - predict(AIC_Log_cd,house_mod)) ^ 2)))
 
 
```

**Discussion on test results for AIC search Model**

The above model built using AIC have the following test results

- $R^2$ value of `r round(summary(AIC_Log_cd)$r.squared,4)`, ie `r percent(summary(AIC_Log_cd)$r.squared)` of the variance in SalePrice is explained by linear relationship with variables in the `BIC model`.

- Normality assumptions and the constant variance assumptions are **valid** for this model which we could see from the QQ plot and fitted vs residual plot.
The high p-value of `BP test` and `Shapiro-wilk test` confirms the same.

- Train RMSE of `r round((RMSE_Train_add = sqrt(mean(resid(AIC_Log_cd) ^ 2))),4)`

- Test RMSE of `r round((RMSE_test_add = sqrt(mean((log(house_tst$SalePrice) - predict(AIC_Log_cd,house_mod)) ^ 2))),4)`

Also, this model has better $R^2$ value and this model has `r length(coef(AIC_Log_cd))`parameters which is less parameters compared to the log interaction model, which make this model more interpretable than the log interaction model.


## Results

Based on the discussion points about $R^2$, RMSE and model assumption discussed in each model section and from the table below we could see that, the model built using the AIC search with log interaction model(AIC_Log_cd) is comparitively the good model for predicting the house SalePrice as it has the best $R^2$ of the 6 models built above and it has comparitively good test RMSE.

```{r,echo=FALSE}

R2_val = c(r2_add, r2_add2, r2_int, r2_cd, r2_bic, r2_aic)
rmse_train = c(RMSE_Train_add, RMSE_Train_add2, RMSE_Train_log_int, RMSE_Train_log_cd, RMSE_Train_BIC_log_cd, RMSE_Train_AIC_log_cd)
rmse_test = c(RMSE_test_add, RMSE_test_add2, RMSE_test_log_int, RMSE_test_log_cd, RMSE_test_BIC_log_cd, RMSE_test_AIC_log_cd)

models = c("add_mod", "add_mod2", "log_int_mod", "int_add_cd_mod", "BIC_Log_cd", "AIC_Log_cd")
results = data.frame(models, R2_val, rmse_train, rmse_test)
colnames(results) = c("Models", "R2 Values", "Train RMSE", "Test RMSE")
knitr::kable(results)

```

The AIC log interaction model also has valid constant variance and normality assumption, which makes the model good for interpretation purpose but it may be  little difficuly to use the AIC_Log_cd model for interpretation, as it has large number of parameters.

The aim of this project is to build a priction model,usually if we are building a models for prediction purpose we can select the model based on the $R^2$ and RMSE values alone as the model will be good for prediction and we shouldn't be concerned about the number of parameter.

## Discussion















