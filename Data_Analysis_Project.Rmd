---
title: "STAT 420: Data Analysis Project - Predict the price of your dream home" 
author: "Anupama Agrahari, Dhanendra Singh, Naveen Kumar Palani, Shanthakumar Subramanian"
date: ''
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---




## Team

We are team of four. Please find below our names and Net ID in alphabetical order.

```{r echo = FALSE, result = 'asis'}
members = matrix(nrow = 4, ncol = 3)
colnames(members) = c("FirstName", "LastName", "NetID")
rownames(members) = c("1", "2", "3","4")

members[1,1] = "Anupama"
members[1,2] = "Agrahari"
members[1,3] = "anupama3"
members[2,1] = "Dhanendra"
members[2,2] = "Singh"
members[2,3] = "disingh2"
members[3,1] = "Naveen Kumar"
members[3,2] = "Palani"
members[3,3] = "npalani3"
members[4,1] = "Shanthakumar"
members[4,2] = "Subramanian"
members[4,3] = "SS81"
knitr::kable(members)
```

## Introduction

### About the Project
This data analysis project is inspired from the House Prices: Advanced Regression Techniques competition held in Kaggle.

In this project we will be try to predict the final price of each home, based on the 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa.

###Data Set

#### Source
 techniques to analyze the data. Below are the details about our dataset.
We are using house prie data available at  https://www.kaggle.com/c/house-prices-advanced-regression-techniques . We will use regression

#### Background

The Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. 


#### File descriptions

List of files used in the project

- train.csv - the training set
- test.csv - the test set
- data_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here

#### Data fields

Here's a brief description of different fields of the house data set.

- SalePrice - the property's sale price in dollars. This is the target variable that we are trying to predict.
- MSSubClass: The building class
- MSZoning: The general zoning classification
- LotFrontage: Linear feet of street connected to property
- LotArea: Lot size in square feet
- Street: Type of road access
- Alley: Type of alley access
- LotShape: General shape of property
- LandContour: Flatness of the property
- Utilities: Type of utilities available
- LotConfig: Lot configuration
- LandSlope: Slope of property
- Neighborhood: Physical locations within Ames city limits
- Condition1: Proximity to main road or railroad
- Condition2: Proximity to main road or railroad (if a second is present)
- BldgType: Type of dwelling
- HouseStyle: Style of dwelling
- OverallQual: Overall material and finish quality
- OverallCond: Overall condition rating
- YearBuilt: Original construction date
- YearRemodAdd: Remodel date
- RoofStyle: Type of roof
- RoofMatl: Roof material
- Exterior1st: Exterior covering on house
- Exterior2nd: Exterior covering on house (if more than one material)
- MasVnrType: Masonry veneer type
- MasVnrArea: Masonry veneer area in square feet
- ExterQual: Exterior material quality
- ExterCond: Present condition of the material on the exterior
- Foundation: Type of foundation
- BsmtQual: Height of the basement
- BsmtCond: General condition of the basement
- BsmtExposure: Walkout or garden level basement walls
- BsmtFinType1: Quality of basement finished area
- BsmtFinSF1: Type 1 finished square feet
- BsmtFinType2: Quality of second finished area (if present)
- BsmtFinSF2: Type 2 finished square feet
- BsmtUnfSF: Unfinished square feet of basement area
- TotalBsmtSF: Total square feet of basement area
- Heating: Type of heating
- HeatingQC: Heating quality and condition
- CentralAir: Central air conditioning
- Electrical: Electrical system
- 1stFlrSF: First Floor square feet
- 2ndFlrSF: Second floor square feet
- LowQualFinSF: Low quality finished square feet (all floors)
- GrLivArea: Above grade (ground) living area square feet
- BsmtFullBath: Basement full bathrooms
- BsmtHalfBath: Basement half bathrooms
- FullBath: Full bathrooms above grade
- HalfBath: Half baths above grade
- Bedroom: Number of bedrooms above basement level
- Kitchen: Number of kitchens
- KitchenQual: Kitchen quality
- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
- Functional: Home functionality rating
- Fireplaces: Number of fireplaces
- FireplaceQu: Fireplace quality
- GarageType: Garage location
- GarageYrBlt: Year garage was built
- GarageFinish: Interior finish of the garage
- GarageCars: Size of garage in car capacity
- GarageArea: Size of garage in square feet
- GarageQual: Garage quality
- GarageCond: Garage condition
- PavedDrive: Paved driveway
- WoodDeckSF: Wood deck area in square feet
- OpenPorchSF: Open porch area in square feet
- EnclosedPorch: Enclosed porch area in square feet
- 3SsnPorch: Three season porch area in square feet
- ScreenPorch: Screen porch area in square feet
- PoolArea: Pool area in square feet
- PoolQC: Pool quality
- Fence: Fence quality
- MiscFeature: Miscellaneous feature not covered in other categories
- MiscVal: $Value of miscellaneous feature
- MoSold: Month Sold
- YrSold: Year Sold
- SaleType: Type of sale
- SaleCondition: Condition of sale

## Methods

###Data Analysis

```{r}
library(lmtest)
library(broom)
library(formattable)
library(faraway)
library(ggplot2)
library(leaps)
library(caret)
```



```{r}
## Import test and train data from the files

train = read.csv("train.csv")
test = read.csv("test.csv")

## Checking the number of columns and observation in test and train data

dim(train)
dim(test)

```

From the dimension of test and train data, we could see that the test data has one less column than the train, as the sale price column wont be availble in test data which we will be predicting as part of this project.

```{r}

str(train)
str(test)

```

From the above str display of data we could see that our data is not completely clean. First, certain variables contain NA values which could cause problems when we make predictive models later on. Second, the levels of some of the factor variables are not the same across the training set and test set.

We also see that some of the factor variables have different levels in train and test data set, which might cause problem in predicting the model.We can make sure the train and test sets have the same factor levels by loading each data set again without converting strings to factors, combining them into one large data set, converting strings to factors for the combined data set and then separating them. 

We also can change any NA in the dataset to None while changing the factor of character columns

```{r}

train = read.csv("train.csv", stringsAsFactors = FALSE) 
test = read.csv("test.csv", stringsAsFactors = FALSE) 

# Remove the target variable not found in test set

SalePrice = train$SalePrice 
train$SalePrice = NULL

# Combine data sets

house_data = rbind(train,test)

# Convert character columns to factor, filling NA values with "None"

for (col in colnames(house_data)){
  if (typeof(house_data[,col]) == "character"){
    new_col = house_data[,col]
    new_col[is.na(new_col)] = "None"
    house_data[col] = as.factor(new_col)
  }
}

```

Now the factor variable should be common across test and train dataset ,lets try to find the missing values in integer variable and fix it

```{r}

## Taking all the missing data indices in one variables

Missing_indices = sapply(house_data,function(x) sum(is.na(x)))
Missing_Summary = data.frame(index = names(house_data),Missing_Values=Missing_indices)
Missing_Summary[Missing_Summary$Missing_Values > 0,]

```

Above are the list of variable having NA values, in the below section, we will fix those values with relavant values

```{r}

house_data$MasVnrArea[which(is.na(house_data$MasVnrArea))] = mean(house_data$MasVnrArea,na.rm=T)

house_data$LotFrontage[which(is.na(house_data$LotFrontage))] = median(house_data$LotFrontage,na.rm = T)

house_data$GarageYrBlt[which(is.na(house_data$GarageYrBlt))] = 0 

house_data$GarageArea[which(is.na(house_data$GarageArea))] = mean(house_data$GarageArea,na.rm=T)

house_data$TotalBsmtSF[which(is.na(house_data$TotalBsmtSF))] = mean(house_data$TotalBsmtSF,na.rm=T)

house_data$BsmtUnfSF[which(is.na(house_data$BsmtUnfSF))] = mean(house_data$BsmtUnfSF,na.rm=T)

house_data$BsmtFinSF1[which(is.na(house_data$BsmtFinSF1))] = mean(house_data$BsmtFinSF1,na.rm=T)

house_data$BsmtFinSF2[which(is.na(house_data$BsmtFinSF2))] = mean(house_data$BsmtFinSF2,na.rm=T)

house_data$GarageCars[which(is.na(house_data$GarageCars))] = mode(house_data$GarageCars)

house_data$BsmtHalfBath[which(is.na(house_data$BsmtHalfBath ))] = mode(house_data$BsmtHalfBath)

house_data$BsmtFullBath[which(is.na(house_data$BsmtFullBath ))] = mode(house_data$BsmtFullBath)

```


separate the train and test data after fixing the NA values

```{r}

# Separate out our train and test sets

train = house_data[1:nrow(train),]
train$SalePrice = SalePrice  
test = house_data[(nrow(train)+1):nrow(house_data),]

```

Lets now have a look at the train data set

```{r}
#str(train)
```

```{r}
#head(train)
```

```{r}
#head(test)
```

**Need to write comments**

```{r}

set.seed(1)
house_trn_idx = sample(1:nrow(train), 1168)
house_trn = train[house_trn_idx,]
house_tst = train[-house_trn_idx,]

```

```{r fig.height=8, fig.width=13}
house_train_num<-house_trn[, sapply(house_trn, is.numeric)]
Correlation<-cor(na.omit(house_train_num))
library(corrplot)
corrplot(Correlation, method = "square")
(col_round = round(cor(Correlation), 2))

```


Now the data looks fine. Lets try to build a additive model with all variable as initial model to do a BIC search on it to find a smaller models with 
appropriate predictiors.

```{r}

col_var = c(,)

for (col in colnames(house_trn)){
   if(is.numeric(house_trn[,col])){
       if( abs(cor(house_trn[,col],house_trn$SalePrice)) > 0.3){
           print(col)
           print( cor(house_trn[,col],house_trn$SalePrice))
         }
   }
}

```

From the above correlation plot, we have selected the below varibles to build the model.

```{r fig.height=7, fig.width=12}
# Histogram expensive are houses?

var_sel = c(SalePrice ,LotFrontage, OverallQual, YearBuilt,YearRemodAdd, MasVnrArea, BsmtFinSF1, TotalBsmtSF, X1stFlrSF, X2ndFlrSF,GrLivArea, FullBath, TotRmsAbvGrd, Fireplaces, GarageArea, WoodDeckSF)

var_sel = c("house_trn$SalePrice" ,"house_trn$LotFrontage")



par(mfrow = c(1,2))
for(i in 1:length(var_sel))
hist(toString(var_sel[i]),
    xlab   = "var_sel",
    main   = "Histogram of sales price",
    border = "dodgerblue",
    col    = "darkorange",
    breaks = 20)



qqnorm(train$SalePrice, main = "Normal Q-Q Plot, without log(SalePrice)", col = "darkgrey")
qqline(train$SalePrice, col = "dodgerblue", lwd = 2)

qqnorm(log(train$SalePrice), main = "Normal Q-Q Plot, with log(SalePrice) ", col = "darkgrey")
qqline(log(train$SalePrice), col = "dodgerblue", lwd = 2)
```



```{r}

```


```{r}
cors = cor(train[ , sapply(train, is.numeric)])
high_cor = which(abs(cors) > 0.6 & (abs(cors) >= 1))
rows = rownames(cors)[((high_cor-1) %/% 38)+1]
cols = colnames(cors)[ifelse(high_cor %% 38 == 0, 38, high_cor %% 38)]
vals = cors[high_cor]

cor_data = data.frame(cols=cols, rows=rows, correlation=vals)
cor_data
```


```{r}
head(train)
```


```{r}
cors = cor(train[ , sapply(train, is.numeric)])
high_cor = which(abs(cors) > 0.6 & (abs(cors) < 1))
rows = rownames(cors)[((high_cor-1) %/% 38)+1]
cols = colnames(cors)[ifelse(high_cor %% 38 == 0, 38, high_cor %% 38)]
vals = cors[high_cor]

cor_data = data.frame(cols=cols, rows=rows, correlation=vals)
cor_data
```

<!-- ```{r} -->
<!-- initial_model = lm(SalePrice~.,data=train) -->
<!-- BIC_selected = step(initial_model,k=log(nrow(train)),trace=0) -->

<!-- ``` -->

<!-- ```{r} -->

<!-- (vif_model <- coef(BIC_selected)[vif(BIC_selected) >5]) -->
<!-- #big_model_fix = lm(mpg ~ disp * hp * domestic, -->
<!-- #data = autompg, -->
<!-- #subset = vif_model < 4 / length(big_mod_cd)) -->
<!-- ``` -->



<!-- ```{r} -->
<!-- #summary(BIC_selected) -->

<!-- BIC_Log_Select <- lm(log(SalePrice) ~ MSSubClass + LotArea + Street + Neighborhood +  -->
<!--     Condition2 + OverallQual + OverallCond + YearBuilt + RoofMatl +  -->
<!--     ExterQual + BsmtQual + BsmtExposure + BsmtFinSF1 + BsmtFinSF2 +  -->
<!--     BsmtUnfSF + X1stFlrSF + X2ndFlrSF + BedroomAbvGr + KitchenAbvGr +  -->
<!--     KitchenQual + TotRmsAbvGrd + GarageCars + ScreenPorch + PoolArea +  -->
<!--     PoolQC + SaleCondition, data = train) -->
<!-- summary(BIC_Log_Select) -->
<!-- ``` -->

<!-- **Checking for multicollineariaty** -->
<!-- ```{r} -->

<!-- multi_col = coef(BIC_selected)[(vif(BIC_selected)>5)] -->
<!-- #length(multi_col) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- par(mfrow = c(2, 2)) -->
<!-- plot(fitted(BIC_selected), resid(BIC_selected), col = "grey", pch = 20, -->
<!--     xlab = "Fitted", ylab = "Residuals", main = "Data from Model 2") -->
<!-- abline(h = 0, col = "darkorange", lwd = 2) -->

<!-- qqnorm(resid(BIC_selected), main = "Normal Q-Q Plot", col = "darkgrey") -->
<!-- qqline(resid(BIC_selected), col = "dodgerblue", lwd = 2) -->

<!-- #par(mfrow = c(2, 2)) -->
<!-- plot(fitted(BIC_Log_Select), resid(BIC_Log_Select), col = "grey", pch = 20, -->
<!--     xlab = "Fitted", ylab = "Residuals", main = "Data from Model 2") -->
<!-- abline(h = 0, col = "darkorange", lwd = 2) -->

<!-- qqnorm(resid(BIC_Log_Select), main = "Normal Q-Q Plot", col = "darkgrey") -->
<!-- qqline(resid(BIC_Log_Select), col = "dodgerblue", lwd = 2) -->

<!-- ``` -->


<!-- ```{r} -->
<!-- bptest(BIC_selected) -->

<!-- bptest(BIC_Log_Select) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- shapiro.test(resid(BIC_selected)) -->
<!-- shapiro.test(resid(BIC_Log_Select)) -->

<!-- ``` -->

## Model Building
- step 0 : Data exploration, cleansing and check for multicollineariaty
- step 1 : Build a additive model with all variables
- step 2 : Use one of the search methode to find out the best model for the additive
- step 3 : check the normal distribution and constant variance assumption
- step 4 : Do predictor or response transformation based on the results of the test in step 3
- step 5 : Finalize the model
- Step 6 : Chek for train and test RMSE
- step 7 : check for leverage and influential observation
- step 8 : Build the model after removing the influential observations(if any) and 
           check the RMSE values
- step 9 : predict the results with the final model

**Note** 
  
  - Everytime we fit the model, we need to check the normal distribution and constant variance assumption
  
**Steps from Video**

- 

Points to consider 

1. Do we need to check LOOCV?


## Results
## Discussion
## Appendix











